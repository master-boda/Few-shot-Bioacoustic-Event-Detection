\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Final Project: Few-Shot Detection of Bioacoustic Events}

\author{
\IEEEauthorblockN{Vicente Jos\'e Escarigo Miranda}
\IEEEauthorblockA{
\textit{UAM, Universidad Autonoma de Madrid} \\
\\
}
}

\maketitle

\begin{abstract}
Few-shot learning is a promising paradigm for bioacoustic sound event detection, 
where only a small number of labeled examples are available for a target species 
or call type. 

This report proposes a lightweight model for DCASE 2023 Task 5 
(Few-shot Bioacoustic Event Detection). The method follows a general pipeline 
that converts raw audio into time--frequency representations, extracts frame-level 
embeddings with a shared encoder, performs support-guided scoring of query audio, 
applies temporal post-processing to form event predictions, and evaluates performance 
using the official event-based F-measure based on IoU and bipartite matching.

The goal of this project is to understand the task constraints in depth and 
deliver a reproducible baseline system that can be extended with alternative 
modeling choices.
\end{abstract}

\begin{IEEEkeywords}
few-shot learning, bioacoustics, sound event detection, embeddings, 
prototypical methods, event-based evaluation
\end{IEEEkeywords}


\section{Introduction}
Few-Shot Learning (FSL) is a machine learning approach where a model is required to adapt
to new, previously unseen categories using only a small number of labeled examples. This 
technique is especially valuable in scenarios where data collection or annotation is 
expensive or very time consuming. One application of this is Sound Event Detection (SED), which 
involves identifying the start and end points of specific sounds within an audio recording. 
In fields like bioacoustics, recordings often span long durations with only a few relevant 
sound events, making manual labeling especially challenging. FSL offers a solution 
by detecting these events using minimal annotated data. The DCASE Task~5
challenge exemplifies this problem under strict constraints: each file provides only five
positive events as support, evaluation ignores everything before the fifth event, and
each file must be treated independently \cite{dcasechallengeDCASE2023Task}. These rules
make the task substantially harder than standard supervised SED and emphasize issues
such as background noise, variable event durations, and domain mismatch across recording
sources.

This project builds on the official DCASE 2023 prototypical network baseline as a reference 
point \cite{snellPrototypicalNetworksFewshot2017a} and experiments on how
far it can be improved without heavy architectures or external data.
The focus is therefore on feasible, secondary improvements such as feature
representation, adaptive windowing, negative sampling strategy, transductive refinement,
and post‑processing.

Although the 2024 edition expands the validation set and updates the official baseline,
the 2023 edition uses the same development data and baseline definition used across
the 2022--2023 technical reports. I therefore use the 2023 edition to keep the reference
baseline fixed and to attribute performance changes to the proposed modifications rather
than to a shifted validation split or baseline update.
\section{State of the Art}
Prototypical Networks introduced an important development approach to few-shot classification through metric 
learning, where the model is trained to map inputs into an embedding space. In this space, 
classification is done by measuring the distance to prototype representations of each 
class, resulting in effective generalization from just a few labeled examples 
\cite{snellPrototypicalNetworksFewshot2017a}. In 2022, most Task~5 systems stayed within
this template, improving the encoder, segmentation strategy, and post‑processing
\cite{liuSURREYSYSTEMDCASE2022,tangFEWSHOTEMBEDDINGLEARNING2022}. By 2023,
the best results came from frame‑level fine‑tuning and auxiliary objectives (e.g., multi‑task
branches) and from contrastive pretraining on the official data
\cite{yanMULTITASKFRAMELEVEL2023,moummadSUPERVISEDCONTRASTIVELEARNING2023}. In 2024, the
trend continued toward stronger backbones and pretraining (e.g., U‑Net variants and
AAPM‑style encoders), while prototypical methods were still strongly present in modified form through
better negatives, attention mechanisms, or hybrid front‑ends \cite{dengWeiLiu1Hy2024}.

\section{Description of the Proposed Method}

\subsection{Overview}
Following a lightweight prototypical‑network setup \cite{snellPrototypicalNetworksFewshot2017a}, 
audio is first converted into time–frequency representations, embedded via a shallow ResNet encoder, and evaluated through distance to class prototypes. Support positives form the positive prototype, while negatives are sampled from silent regions between events \cite{tangFEWSHOTEMBEDDINGLEARNING2022}. Query embeddings are scored using softmax-based distances and converted to events via thresholding and refinement.

\subsection{Feature Representation}
We adopt a dual representation of PCEN and delta‑MFCCs to enhance robustness against channel and background variability. PCEN acts as a compressive, adaptive gain control \cite{wangTrainableFrontendRobust2016}, while delta‑MFCCs capture temporal dynamics \cite{hossanNovelApproachMFCC2011}. This combination yielded strong validation performance in DCASE 2022 Task 5 \cite{liuSURREYSYSTEMDCASE2022}.

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{figs/fig_features_triplet.pdf}
\caption{Example input representations: log‑mel, PCEN, and delta‑MFCC.}
\label{fig:feature_triplet}
\end{figure}

\subsection{Segmentation Strategy}
Training uses fixed-length segments, while inference adapts the segment length per file to improve alignment with variable event durations \cite{tangFEWSHOTEMBEDDINGLEARNING2022}. We compute the mean duration of the five positive supports and derive both segment length and hop size from this.

\begin{equation}
\begin{aligned}
\bar{t} &= \frac{1}{K}\sum_{i=1}^{K} (t^{\text{end}}_i - t^{\text{start}}_i) \\
L &= \min\big(L_{\max}, \lfloor \bar{t} \cdot f_s \rfloor\big), \quad
H = \lfloor L / 2 \rfloor
\end{aligned}
\end{equation}

\subsection{Encoder Architecture}
The encoder is a shallow ResNet using three residual blocks with progressively increasing channel sizes. Residual connections enable deeper representation without overfitting.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth]{figs/residual_block.pdf}
\caption{Residual block used in the ResNet encoder.}
\label{fig:residual_block}
\end{figure}

\begin{table}[t]
\caption{Encoder layer overview. Each residual block contains three 3$\times$3 convolutions.}
\label{tab:resnet_stages}
\centering
\begin{tabular}{lcc}
\hline
\textbf{Layers} & \textbf{Channels} & \textbf{Kernel size} \\
\hline
ResidualBlock & 64 & 3$\times$3 \\
ResidualBlock & 128 & 3$\times$3 \\
ResidualBlock & 64 & 3$\times$3 \\
AdaptiveAvgPooling & -- & 4$\times$2 \\
\hline
\end{tabular}
\end{table}

\subsection{Training Paradigm}
We adopt episodic training under a 5-way, 5-shot few-shot setup. Each training episode samples supports and queries from five classes, optimizing the encoder using a prototypical objective.

\begin{figure}[t]
\centering
\makebox[\columnwidth][c]{\includegraphics[width=1.3\columnwidth]{figs/episodic_training.pdf}}
\caption{Episodic training schematic (C‑way (5), K‑shot (2)).}
\label{fig:episodic_training}
\end{figure}

\subsection{Prototype Scoring and Transductive Refinement}
We compute positive and negative prototypes as class centroids in embedding space:
\begin{equation}
\mathbf{p}_{+}=\frac{1}{K}\sum_{i=1}^{K} f_\theta(\mathbf{x}^{+}_i), \quad
\mathbf{p}_{-}=\frac{1}{M}\sum_{j=1}^{M} f_\theta(\mathbf{x}^{-}_j)
\end{equation}
The classification is done via:
\begin{equation}
P(y=+ \mid \mathbf{z}) =
\mathrm{softmax}\big(-d(\mathbf{z},\mathbf{p}_{+}),\,-d(\mathbf{z},\mathbf{p}_{-})\big)_+
\end{equation}
A transductive step optionally adjusts prototypes using query statistics \cite{boudiafFewShotSegmentationMetaLearning2021}.

\subsection{Post‑Processing Logic}
Event predictions are refined using heuristic filtering. Short events are removed, and closely spaced detections are merged. Thresholds for duration and gap filtering are derived from the average support duration \cite{hertkornFEWSHOTBIOACOUSTICEVENT2022}:
\begin{equation}
\begin{aligned}
\text{keep event} &:\ (t^{\text{end}}-t^{\text{start}}) \ge \alpha\,\bar{t} \\
\text{merge if} &:\ (t^{\text{start}}_{i+1}-t^{\text{end}}_{i}) \le \beta\,\bar{t}
\end{aligned}
\end{equation}
\subsection{Overview}
Following a lightweight prototypical‑network setup \cite{snellPrototypicalNetworksFewshot2017a}, 
audio is converted to PCEN features with delta‑MFCCs \cite{wangTrainableFrontendRobust2016,hossanNovelApproachMFCC2011} 
and embedded by a shallow ResNet encoder \cite{heDeepResidualLearning2016}. The five positives 
define a positive prototype, while negatives are taken from the gaps between positives as 
suggested by Tang et al. \cite{tangFEWSHOTEMBEDDINGLEARNING2022}. Query segments are scored 
by distance to the positive and negative prototypes and then converted into events based
on sigmoid activation, followed by post-processing.
\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{figs/pipeline.pdf}
\caption{Pipeline overview.}
\label{fig:pipeline}
\end{figure}

\subsection{Feature extraction}
All audio is resampled to 22.05\,kHz and converted to 128‑bin mel spectrograms
($n\_fft=512$, hop=256). Per‑Channel Energy Normalization (PCEN) is applied, an
automatic gain control followed by compression that improves robustness to channel
variability and background noise \cite{wangTrainableFrontendRobust2016}. In parallel, I 
compute MFCCs and their temporal derivatives (delta‑MFCC) to capture short‑term
spectral dynamics \cite{hossanNovelApproachMFCC2011}. Liu et al. reported that
PCEN concatenated with delta‑MFCC gave the best average validation performance in
DCASE2022 Task~5, so I adopt the same input representation \cite{liuSURREYSYSTEMDCASE2022}.

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{figs/fig_features_triplet.pdf}
\caption{Example input representations: log‑mel, PCEN, and delta‑MFCC.}
\label{fig:feature_triplet}
\end{figure}

\subsection{Segmentation}
For training, fixed‑length segments of 0.2\,s are extracted with 0.1\,s hop.
At evaluation, segment length is adapted to each file using the mean duration
of the five positive events, while the hop length is set as a fraction of that
window. Tang et al. showed that adapting window length to the positive event
durations improves resolution for variable‑length calls while keeping enough
context for matching \cite{tangFEWSHOTEMBEDDINGLEARNING2022}. I follow that strategy.

\subsection{Encoder and episodic training}
I use a shallow ResNet encoder with residual blocks \cite{heDeepResidualLearning2016} and
train it episodically in a 5‑way, 5‑shot setting, as in the prototypical learning
paradigm \cite{snellPrototypicalNetworksFewshot2017a}. Each episode samples positive and
query segments, and the encoder is optimized with Adam ($\mathrm{lr}=10^{-3}$, 2000 episodes
per epoch).

\begin{figure}[t]
\centering
\makebox[\columnwidth][c]{\includegraphics[width=1.3\columnwidth]{figs/episodic_training.pdf}}
\caption{Episodic training schematic (C‑way (5), K‑shot (2)).}
\label{fig:episodic_training}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth]{figs/residual_block.pdf}
\caption{Residual block used in the ResNet encoder.}
\label{fig:residual_block}
\end{figure}

\begin{table}[t]
\caption{Encoder layer overview. Each residual block contains three 3$\times$3 convolutions.}
\label{tab:resnet_stages}
\centering
\begin{tabular}{lcc}
\hline
\textbf{Layers} & \textbf{Channels} & \textbf{Kernel size} \\
\hline
ResidualBlock & 64 & 3$\times$3 \\
ResidualBlock & 128 & 3$\times$3 \\
ResidualBlock & 64 & 3$\times$3 \\
AdaptiveAvgPooling & -- & 4$\times$2 \\
\hline
\end{tabular}
\end{table}

\subsection{Prototype scoring and transductive refinement}
For each evaluation file, the positive prototype is the mean embedding of the five
supports. The negative prototype is estimated from segments sampled in the gaps
between positives; Tang et al. reported this reduces positive contamination versus
random negatives in dense‑event files \cite{tangFEWSHOTEMBEDDINGLEARNING2022}. To reduce randomness, I sample negatives
multiple times and average the resulting scores. I optionally perform a transductive
refinement step that re‑weights prototypes using query embeddings, similar in spirit
to transductive information maximization in few‑shot learning
\cite{boudiafFewShotSegmentationMetaLearning2021}.

\subsection{Post‑processing}
Frame‑level probabilities are thresholded, then short detections are filtered
using a minimum duration derived from the support events. Neighboring detections
separated by very short gaps are merged. Hertkorn et al. showed that duration‑based
filtering and gap‑merging can suppress bursts of false positives in Task~5 evaluation,
so I use similar rules here \cite{hertkornFEWSHOTBIOACOUSTICEVENT2022}.

\section{Experimental Setup}

\subsection{Dataset overview}
Experiments use the DCASE 2023 Task~5 development set \cite{dcasechallengeDCASE2023Task}.
The Training\_Set aggregates five sources (BV, HT, JD, MT, WMW) with multi‑class
annotations; the official totals are 174 recordings, 21\,h, 47 classes, and 14\,229
positive events. The Validation\_Set in the dev package used here contains three
subsets (HB, PB, ME) totaling 18 recordings and 1\,034 positive events. Sampling
rates range from 6\,kHz to 48\,kHz, and there is no class overlap between training
and validation.

\begin{table}[!t]
\caption{Training set overview (official statistics).}
\label{tab:train_overview}
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Subset} & \textbf{Recordings} & \textbf{Duration} & \textbf{Classes} & \textbf{Events} \\
\hline
BV  & 5   & 10\,h        & 11 & 9\,026 \\
HT  & 5   & 5\,h         & 3  & 611 \\
JD  & 1   & 10\,m        & 1  & 357 \\
MT  & 2   & 1\,h\,10\,m  & 4  & 1\,294 \\
WMW & 161 & 4\,h\,40\,m  & 26 & 2\,941 \\
\hline
\textbf{Total} & \textbf{174} & \textbf{21\,h} & \textbf{47} & \textbf{14\,229} \\
\hline
\end{tabular}
\end{table}

\begin{table}[!t]
\caption{Validation set overview (local dev package).}
\label{tab:val_overview}
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Subset} & \textbf{Recordings} & \textbf{Duration} & \textbf{Classes} & \textbf{Events} \\
\hline
HB & 10 & 2\,h\,38\,m & 1 & 712 \\
PB & 6  & 3\,h        & 2 & 260 \\
ME & 2  & 20\,m       & 2 & 62 \\
\hline
\textbf{Total} & \textbf{18} & \textbf{5\,h\,57\,m} & \textbf{5} & \textbf{1\,034} \\
\hline
\end{tabular}
\end{table}

\subsection{Protocol and Annotations}
Validation uses a 5-shot protocol. Only the first five positive events in each file are used as support. Evaluation ignores data before the fifth event ends. Unknown (UNK) segments are excluded.

\subsection{Feature Extraction}
Audio is resampled to 22.05\,kHz. 128-bin mel spectrograms are computed with $n\_fft=512$ and hop=256. PCEN is applied, and 40-dimensional delta‑MFCCs are concatenated.

\subsection{Model Training}
Training segments are 0.2\,s with 0.1\,s hop. The encoder is trained for 15 epochs with 2000 episodes per epoch using Adam ($\mathrm{lr}=10^{-3}$) and a StepLR scheduler (step size 10, gamma 0.5). Validation accuracy selects the best checkpoint.

\subsection{Evaluation Configuration}
At test time, segment length is adapted per file using the maximum support event duration (capped at 20 frames); hop is set to half the segment length. Negative prototypes are averaged from 30 samples over 6 iterations. Transductive refinement uses 2 steps (temperature 1.0, weight 0.1). Post‑processing applies:
- Threshold = 0.6
- Minimum duration = $0.2 \times \bar{t}$
- Merge gap = $0.05 \times \bar{t}$

\subsection{Evaluation Metric}
Performance is measured with the official macro-averaged F-measure using event-based IoU and bipartite matching. Support regions are excluded.

\subsection{Reproducibility}
Training and

\section{Results}
Results will be reported using the official event-based F-measure, alongside
precision and recall for diagnostic analysis.

\section{Discussion and Conclusion}
This project proposes a reproducible baseline framework for DCASE 2023 Task 5
that emphasizes correctness, modularity, and computational efficiency.

By implementing the full pipeline from audio to event predictions and official
evaluation, the project will establish a strong foundation for exploring
alternative encoders and few-shot scoring strategies while remaining aligned
with the task constraints.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
