@misc{boudiafFewShotSegmentationMetaLearning2021,
  title = {Few-{{Shot Segmentation Without Meta-Learning}}: {{A Good Transductive Inference Is All You Need}}?},
  shorttitle = {Few-{{Shot Segmentation Without Meta-Learning}}},
  author = {Boudiaf, Malik and Kervadec, Hoel and Masud, Ziko Imtiaz and Piantanida, Pablo and Ayed, Ismail Ben and Dolz, Jose},
  year = 2021,
  month = mar,
  number = {arXiv:2012.06166},
  eprint = {2012.06166},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.06166},
  urldate = {2026-01-13},
  abstract = {We show that the way inference is performed in few-shot segmentation tasks has a substantial effect on performances -- an aspect often overlooked in the literature in favor of the meta-learning paradigm. We introduce a transductive inference for a given query image, leveraging the statistics of its unlabeled pixels, by optimizing a new loss containing three complementary terms: i) the cross-entropy on the labeled support pixels; ii) the Shannon entropy of the posteriors on the unlabeled query-image pixels; and iii) a global KL-divergence regularizer based on the proportion of the predicted foreground. As our inference uses a simple linear classifier of the extracted features, its computational load is comparable to inductive inference and can be used on top of any base training. Foregoing episodic training and using only standard cross-entropy training on the base classes, our inference yields competitive performances on standard benchmarks in the 1-shot scenarios. As the number of available shots increases, the gap in performances widens: on PASCAL-5i, our method brings about 5\% and 6\% improvements over the state-of-the-art, in the 5- and 10-shot scenarios, respectively. Furthermore, we introduce a new setting that includes domain shifts, where the base and novel classes are drawn from different datasets. Our method achieves the best performances in this more realistic setting. Our code is freely available online: https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/PHJG27DJ/Boudiaf et al. - 2021 - Few-Shot Segmentation Without Meta-Learning A Good Transductive Inference Is All You Need.pdf;/home/vic/snap/zotero-snap/common/Zotero/storage/TPAYTUDQ/2012.html}
}

@misc{dcasechallengeDCASE2023Task,
  title = {{{DCASE}} 2023 {{Task}} 5},
  author = {DCASE Challenge},
  howpublished = {https://dcase.community/challenge2023/task-few-shot-bioacoustic-event-detection\#development-set}
}

@misc{DcasefewshotbioacousticEvaluation_metricsMain,
  title = {Dcase-Few-Shot-Bioacoustic/Evaluation\_metrics at Main {$\cdot$} C4dm/Dcase-Few-Shot-Bioacoustic},
  journal = {GitHub},
  urldate = {2026-01-15},
  abstract = {Contribute to c4dm/dcase-few-shot-bioacoustic development by creating an account on GitHub.},
  howpublished = {https://github.com/c4dm/dcase-few-shot-bioacoustic/tree/main/evaluation\_metrics},
  langid = {english},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/8FAH5UH3/evaluation_metrics.html}
}

@article{dengWeiLiu1Hy2024,
  title = {Wei {{Liu1}}, {{Hy Liu1}}, {{Fl Lin1}}, {{Hs Liu1}}, {{Tian Gao1}}, {{Xin Fang1}}, {{Jh Liu}}},
  author = {Deng, Xuyao and Sun, Yanjie and Xu, Kele and Dou, Yong},
  year = 2024,
  abstract = {In this technical report, we describe the submission system for DCASE2024 Task 5: Few-shot Bioacoustic Event Detection. In previous work, we proposed a frame-level embedding learning system and achieved the best performance in DCASE2022 Task 5. In this task, we propose several methods to improve the representational capacity of embeddings under limited positive samples. Three methods are proposed based on the pre-training fine-tuning process, including the AAPM segment-level embedding learning method, the Baseline framework-level embedding learning method, and the Unet network-based framework-level embedding learning method. Compared to our previous work, our new system achieved better results on the official 2023 validation set (F-measure 76.8\%, No ML). The proposed system was evaluated on the newly released official 2024 validation set, with a best overall F-measure score of 70.56\%.},
  langid = {english},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/XDL6P3E2/Deng et al. - 2024 - Wei Liu1, Hy Liu1, Fl Lin1, Hs Liu1, Tian Gao1, Xin Fang1, Jh Liu.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = 2016,
  month = jun,
  pages = {770--778},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2026-01-13},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/SCZEJ8G7/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@article{hertkornFEWSHOTBIOACOUSTICEVENT2022,
  title = {{{FEW-SHOT BIOACOUSTIC EVENT DETECTION}}: {{DON}}'{{T WASTE INFORMATION}}},
  author = {Hertkorn, Michael},
  year = 2022,
  abstract = {In the past a lot of attention has been dedicated into finding a good neural network architecture, mainly adopting large NN architectures found in image processing.[1] The parameters in the fixed preprocess, which usually consists of a short-time Fourier transform (STFT) and optionally adding a Mel or Mel frequency cepstral coefficient (MFCC) transformation, can be made trainable[2], however some major parameters stay fixed, like the window size and the fact that the absolute of the complex output of the Fourier transformation is calculated. Also, a learnable frontend is not desirable for a few-shot training setting.},
  langid = {english},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/VDW5QLAZ/Hertkorn - 2022 - FEW-SHOT BIOACOUSTIC EVENT DETECTION DONâ€™T WASTE INFORMATION.pdf}
}

@inproceedings{hossanNovelApproachMFCC2011,
  title = {A Novel Approach for {{MFCC}} Feature Extraction},
  author = {Hossan, Md and Memon, Sheeraz and Gregory, Mark},
  year = 2011,
  month = jan,
  pages = {1--5},
  doi = {10.1109/ICSPCS.2010.5709752},
  abstract = {The Mel-Frequency Cepstral Coefficients (MFCC) feature extraction method is a leading approach for speech feature extraction and current research aims to identify performance enhancements. One of the recent MFCC implementations is the Delta-Delta MFCC, which improves speaker verification. In this paper, a new MFCC feature extraction method based on distributed Discrete Cosine Transform (DCT-II) is presented. Speaker verification tests are proposed based on three different feature extraction methods including: conventional MFCC, Delta-Delta MFCC and distributed DCT-II based Delta-Delta MFCC with a Gaussian Mixture Model (GMM) classifier.},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/HFGW9P8E/Hossan et al. - 2011 - A novel approach for MFCC feature extraction.pdf}
}

@article{liuSURREYSYSTEMDCASE2022,
  title = {{{SURREY SYSTEM FOR DCASE}} 2022 {{TASK}} 5: {{FEW-SHOT BIOACOUSTIC EVENT DETECTION WITH SEGMENT-LEVEL METRIC LEARNING}}},
  author = {Liu, Haohe and Liu, Xubo and Mei, Xinhao and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D},
  year = 2022,
  abstract = {Few-shot audio event detection is a task that detects the occurrence time of a novel sound class given a few examples. In this work, we propose a system based on segment-level metric learning for DCASE 2022 challenge few-shot bioacoustic event detection (task 5). We make better utilization of the negative data within each sound class to build the loss function, and use transductive inference to gain better adaptation on the evaluation set. For the input feature, we find the per-channel energy normalization concatenated with delta melfrequency cepstral coefficients to be the most effective combination. We also introduce new data augmentation and post-processing procedures for this task. Our final system achieves an f-measure of 68.74 on the DCASE task 5 validation set, outperforming the baseline performance of 29.5 by a large margin. Our system is fully open-sourced1.},
  langid = {english},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/6T2TBZLA/Liu et al. - 2022 - SURREY SYSTEM FOR DCASE 2022 TASK 5 FEW-SHOT BIOACOUSTIC EVENT DETECTION WITH SEGMENT-LEVEL METRIC.pdf}
}

@article{moummadSUPERVISEDCONTRASTIVELEARNING2023,
  title = {{{SUPERVISED CONTRASTIVE LEARNING FOR PRE-TRAINING BIOACOUSTIC FEW-SHOT SYSTEMS}}},
  author = {Moummad, Ilyass and Serizel, Romain and Farrugia, Nicolas},
  year = 2023,
  langid = {english},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/AUXV6LH3/Moummad et al. - 2023 - SUPERVISED CONTRASTIVE LEARNING FOR PRE-TRAINING BIOACOUSTIC FEW-SHOT SYSTEMS.pdf}
}

@misc{snellPrototypicalNetworksFewshot2017,
  title = {Prototypical {{Networks}} for {{Few-shot Learning}}},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
  year = 2017,
  month = jun,
  number = {arXiv:1703.05175},
  eprint = {1703.05175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.05175},
  urldate = {2026-01-13},
  abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/XXW8IBGI/Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf;/home/vic/snap/zotero-snap/common/Zotero/storage/QK55ZLPT/1703.html}
}

@article{snellPrototypicalNetworksFewshot2017a,
  title = {Prototypical {{Networks}} for {{Few-shot Learning}}},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
  year = 2017,
  month = jun,
  eprint = {1703.05175},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1703.05175},
  urldate = {2026-01-13},
  abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/GL5ZNMW3/Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf;/home/vic/snap/zotero-snap/common/Zotero/storage/3RWA7FZP/1703.html}
}

@article{tangFEWSHOTEMBEDDINGLEARNING2022,
  title = {{{FEW-SHOT EMBEDDING LEARNING AND EVENT FILTERING FOR BIOACOUSTIC EVENT DETECTION}}},
  author = {Tang, Jigang and Zhang, Xueyang and Gao, Tian and Liu, Diyuan and Fang, Xin and Pan, Jia and Wang, Qing and Du, Jun and Xu, Kele and Pan, Qinghua},
  year = 2022,
  abstract = {In this technical report, we describe our submission system for DCASE2022 Task5: few-shot bioacoustic event detection. We propose several methods to improve the representational ability of embedding under limited positive samples. Including the segmentlevel and frame-level embedding learning strategy, model adaptation technology and embedding-guided event filtering approach. The event filtering task is independently trained on each test file to improve the discrimination of embeddings between similar events. The proposed system is evaluated on the official validation set, and the best overall F-measure score is 74.4\%.},
  langid = {english},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/VN7SJLJM/Tang et al. - 2022 - FEW-SHOT EMBEDDING LEARNING AND EVENT FILTERING FOR BIOACOUSTIC EVENT DETECTION.pdf}
}

@article{wangTrainableFrontendRobust2016,
  title = {Trainable {{Frontend For Robust}} and {{Far-Field Keyword Spotting}}},
  author = {Wang, Yuxuan and Getreuer, Pascal and Hughes, Thad and Lyon, Richard F. and Saurous, Rif A.},
  year = 2016,
  month = jul,
  eprint = {1607.05666},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1607.05666},
  urldate = {2026-01-13},
  abstract = {Robust and far-field speech recognition is critical to enable true hands-free communication. In far-field conditions, signals are attenuated due to distance. To improve robustness to loudness variation, we introduce a novel frontend called per-channel energy normalization (PCEN). The key ingredient of PCEN is the use of an automatic gain control based dynamic compression to replace the widely used static (such as log or root) compression. We evaluate PCEN on the keyword spotting task. On our large rerecorded noisy and far-field eval sets, we show that PCEN significantly improves recognition performance. Furthermore, we model PCEN as neural network layers and optimize high-dimensional PCEN parameters jointly with the keyword spotting acoustic model. The trained PCEN frontend demonstrates significant further improvements without increasing model complexity or inference-time cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/5BK8QIM5/Wang et al. - 2016 - Trainable Frontend For Robust and Far-Field Keyword Spotting.pdf;/home/vic/snap/zotero-snap/common/Zotero/storage/R8CANBYW/1607.html}
}

@article{yanMULTITASKFRAMELEVEL2023,
  title = {{{MULTI-TASK FRAME LEVEL SYSTEM FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION}}},
  author = {Yan, Genwei and Wang, Ruoyu and Zou, Liang and Du, Jun and Wang, Qing and Gao, Tian and Fang, Xin},
  year = 2023,
  abstract = {This technical report describes our new frame-level embedding learning system for DCASE2023 Task5: few-shot bioacoustic event detection. In the previous work, we proposed the frame-level embedding learning system and achieved the best performance of the DCASE 2022 Task5. In this work, we utilize several techniques to improve upon our previous work. Additionally, we introduce multi-task learning and Target Speaker Voice Activity Detection (TS-VAD) strategies to transform our previous system into a new multi-task frame-level embedding learning system. Compare to our previous work, our new system can achieve a better result (Fmeasure 75.74\%, No ML) on the official validation set.},
  langid = {english},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/4SMJXEE8/Yan et al. - 2023 - MULTI-TASK FRAME LEVEL SYSTEM FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION.pdf}
}

@inproceedings{yeFewShotLearningEmbedding2020,
  title = {Few-{{Shot Learning}} via {{Embedding Adaptation With Set-to-Set Functions}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ye, Han-Jia and Hu, Hexiang and Zhan, De-Chuan and Sha, Fei},
  year = 2020,
  month = jun,
  pages = {8805--8814},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00883},
  urldate = {2026-01-15},
  abstract = {Learning with limited data is a key challenge for visual recognition. Many few-shot learning methods address this challenge by learning an instance embedding function from seen classes and apply the function to instances from unseen classes with limited labels. This style of transfer learning is task-agnostic: the embedding function is not learned optimally discriminative with respect to the unseen classes, where discerning among them leads to the target task. In this paper, we propose a novel approach to adapt the instance embeddings to the target classification task with a set-to-set function, yielding embeddings that are task-specific and are discriminative. We empirically investigated various instantiations of such set-to-set functions and observed the Transformer is most effective --- as it naturally satisfies key properties of our desired model. We denote this model as FEAT (few-shot embedding adaptation w/ Transformer) and validate it on both the standard few-shot classification benchmark and four extended few-shot learning settings with essential use cases, i.e., cross-domain, transductive, generalized few-shot learning, and low-shot learning. It archived consistent improvements over baseline models as well as previous methods, and established the new stateof-the-art results on two benchmarks.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-7168-5},
  langid = {english},
  file = {/home/vic/snap/zotero-snap/common/Zotero/storage/Q4U2FTAQ/Ye et al. - 2020 - Few-Shot Learning via Embedding Adaptation With Set-to-Set Functions.pdf}
}
